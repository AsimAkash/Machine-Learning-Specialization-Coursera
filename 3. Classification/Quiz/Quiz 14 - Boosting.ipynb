{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 14 - Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_11 Questions_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Which of the following is NOT an ensemble method?\n",
    "\n",
    "* A. Gradient boosted trees\n",
    "\n",
    "* B. AdaBoost\n",
    "\n",
    "* C. Random forests\n",
    "\n",
    "* D. Single decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Each binary classifier in an ensemble makes predictions on an input x as listed in the table below. Based on the ensemble coefficients also listed in the table, what is the final ensemble model's prediction for x?\n",
    "\n",
    "    Classifier    coefficient w_t    prediction for x\n",
    "        1              0.61                1\n",
    "        2              0.53               -1\n",
    "        3              0.88               -1\n",
    "        4              0.34                1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. (True/False) Boosted decision stumps is a linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. (True/False) Boosted trees tend to be more robust to overfitting than decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. (True/False) AdaBoost focuses on data points it incorrectly predicted by increasing those weights in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. (True/False) For AdaBoost, test error is an appropriate criterion for choosing the optimal number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Which of the following conditions must be true in order for w_t > 0?\n",
    "\n",
    "* A. weighted_error(f_t) < .25\n",
    "\n",
    "* B. weighted_error(f_t) < .5\n",
    "\n",
    "* C. weighted_error(f_t) > .75\n",
    "\n",
    "* D. weighted_error(f_t) > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_t for weighted_error(f_t) < .25 :  0.69314718056\n",
      "w_t for weighted_error(f_t) < .50 :  0.202732554054\n",
      "w_t for weighted_error(f_t) > .75 :  -0.69314718056\n",
      "w_t for weighted_error(f_t) > .50 :  -0.202732554054\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print \"w_t for weighted_error(f_t) < .25 : \", 0.5*math.log(0.8/0.2)\n",
    "print \"w_t for weighted_error(f_t) < .50 : \", 0.5*math.log(0.6/0.4)\n",
    "print \"w_t for weighted_error(f_t) > .75 : \", 0.5*math.log(0.2/0.8)\n",
    "print \"w_t for weighted_error(f_t) > .50 : \", 0.5*math.log(0.4/0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. In an iteration in AdaBoost, recall that learning the coefficient w_t for learned weak learner f_t is calculated by\n",
    "\n",
    "<img src=\"images/image_14_0.png\">\n",
    "\n",
    "If the weighted error of f_t is equal to .25, what is the value of w_t? Round your answer to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_t for weighted_error(f_t) = 0.25 :  0.549306144334\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print \"w_t for weighted_error(f_t) = 0.25 : \", 0.5*math.log(0.75/0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Which of the following classifiers is most accurate as computed on a weighted dataset? A classifier with:\n",
    "\n",
    "* A. weighted error = 0.1\n",
    "\n",
    "* B. weighted error = 0.3\n",
    "\n",
    "* C. weighted error = 0.5\n",
    "\n",
    "* D. weighted error = 0.7\n",
    "\n",
    "* E. weighted error = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. If you were using AdaBoost and in an iteration of the algorithm were faced with the following classifiers, which one would you be more inclined to include in the ensemble? A classifier with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted error = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Imagine we are training a decision stump in an iteration of AdaBoost, and we are at a node. Each data point is (x1, x2, y), where x1,x2 are features, and y is the label. Also included are the weights of the data. The data at this node is:\n",
    "\n",
    "    Weight    x1    x2     y\n",
    "     0.3       0     1     1\n",
    "     0.35      1     0    -1\n",
    "     0.1       0     1     1\n",
    "     0.25      1     1     1\n",
    "     \n",
    "### 7a. Suppose we split on feature x2. Calculate the weighted error of this split. Round your answer to 2 decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Suppose we assign the same class label to all data in this node. (Pick the class label with the greater total weight.) What is the weighted error at the node? Round your answer to 2 decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. After each iteration of AdaBoost, the weights on the data points are typically normalized to sum to 1. This is used because\n",
    "\n",
    "* A. of issues with numerical instability (underflow/overflow)\n",
    "\n",
    "* B. the weak learners can only learn with normalized weights\n",
    "\n",
    "* C. none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Consider the following 2D dataset with binary labels.\n",
    "\n",
    "<img src=\"images/image_14_1.png\",width=\"25%\" height=\"25%\">\n",
    "\n",
    "We train a series of weak binary classifiers using AdaBoost. In one iteration, the weak binary classifier produces the decision boundary as follows:\n",
    "\n",
    "<img src=\"images/image_14_2.png\",width=\"25%\" height=\"25%\">\n",
    "\n",
    "Which of the five points (indicated in the second figure) will receive higher weight in the following iteration? Choose all that apply.\n",
    "\n",
    "* A. (1)\n",
    "\n",
    "* B. (2)\n",
    "\n",
    "* C. (3)\n",
    "\n",
    "* D. (4)\n",
    "\n",
    "* E. (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B,C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Suppose we are running AdaBoost using decision tree stumps. At a particular iteration, the data points have weights according the figure. (Large points indicate heavy weights.)\n",
    "\n",
    "<img src=\"images/image_14_3.png\", width=\"25%\" height=\"25%\">\n",
    "\n",
    "Which of the following decision tree stumps is most likely to be fit in the next iteration?\n",
    "\n",
    "* A. <img src=\"images/image_14_4.png\",width=\"25%\" height=\"25%\">\n",
    "* B. <img src=\"images/image_14_5.png\",width=\"25%\" height=\"25%\">\n",
    "* C. <img src=\"images/image_14_6.png\",width=\"25%\" height=\"25%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11a. (True/False) AdaBoost can boost any kind of classifier, not just a decision tree stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11b.  (True/False) AdaBoost achieves zero training error after a sufficient number of iterations, as long as we can find weak learners that perform better than random chance at each iteration of AdaBoost (i.e., on weighted data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
